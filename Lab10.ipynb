{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoria 10 - inne istotne zagadnienia NLP\n",
    "\n",
    "Dzisiejsze laboratoria kończą cykl ocenianych zadań. Jest to też dobra okazja do przećwiczenia problemów, które co prawda nie były poruszane na wykładzie, ale są często spotykane w pracy.\n",
    "\n",
    "Dzisiaj skupimy się na trzech aspektach przetwarzania języka:\n",
    "<ol>\n",
    "    <li>Automatycznej sumaryzacji poprzez identyfikację zdań kluczowych.</li>\n",
    "    <li>Ekstrakcji relacji z tekstu nieustrukturyzowanego.</li>\n",
    "    <li>Zagadnieniu Topic Modelling</li>\n",
    "</ol>\n",
    "\n",
    "Ponieważ będziemy intensywnie korzystać z biblioteki SpaCy - załadujmy model poprzez uruchomienie kodu poniżej tak, aby nie trzeba było go ładować w kolejnych komórkach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatyczna sumaryzacja\n",
    "\n",
    "Obecnie, ilość informacji znajdujących się w sieci jest tak przytłaczająca, że nie jesteśmy w stanie czytać tyle ile chcielibyśmy. Dlatego też zaczęły rozwijać się metody automatycznej sumaryzacji tekstu, które z długich dokumentów \"wyciągną\" najistotniejsze fragmenty.\n",
    "\n",
    "Istnieją przeróżne podejścia do tego problemu, kilka z nich wymieniono poniżej\n",
    "<ul>\n",
    "    <li>Wybierz pierwszy paragraf - artykuły często zawierają wszystkie najważniejsze informacje już we wprowadzeniu! (szczególnie te w czasopismach)</li>\n",
    "    <li>Wybierz najistotniejsze zdania, które zawierają najwięcej informacji i tylko je przedstaw użytkownikowi</li>\n",
    "    <li>Staraj się zrozumieć tekst, zamodeluj wiedzę z tekstu i wygeneruj własne zdania. </li>\n",
    "</ul>\n",
    "Na dzisiejszych laboratoriach skupimy się na drugiej z metod - Podzielimy dokument na zdania, a następnie stworzymy ranking istotności zdań, z którego wybierzemy kilka pierwszych (najlepiej ocenionych) elementów. Pierwsza jest zbyt prosta do implementacji na laboratoriach, trzecia - zbyt skomplikowana.\n",
    "\n",
    "Aby stworzyć ranking zdań, można przyjąć następującą strategię:\n",
    "<ol>\n",
    "    <li>Przeprowadź preprocessing tekstu: spraw, aby tekst nie używał wielkich liter.</li>\n",
    "    <li>Podziel tekst na zdania, a następnie każde z tych zdań na słowa.</li>\n",
    "    <li>Ze zbioru wszystkich zdań - stwórz słownik, który każdemu słowu z tekstu przypisze liczbę wystąpień tego słowa w całym tekście (nie tylko w pojedynczym zdaniu!) (słownik powinien być w zmiennej freq). Słowo powinno znaleźć się w słowniku jeśli nie należy do zbioru stopwords (najczęstsze słowa typu: and, or, a, an) i jeśli nie jest znakiem interpunkcyjnym.</li>\n",
    "    <li>Korzystając ze zbudowanego w poprzednim kroku słownika - nadaj każdemu zdaniu wartość oznaczającą jego informatywność. Informatywność zdania może być obliczona jako suma częstości odczytanych ze słownika z poprzedniego kroku (jeśli słowo występuje w słowniku, gdyż słownik nie zawiera stopwordsów!).</li>\n",
    "    <li>Mając stworzony ranking - wybierz top N elementów i przedstaw je jako podsumowanie.</li>\n",
    "</ol>\n",
    "\n",
    "Przesłanka do tego podejścia jest taka, że jeśli dane słowo (które nie należy do stopwords) występuje często - jest pewnie istotne. Jeśli w zdaniu występuje dużo istotnych słów - zdanie jest lepsze z punktu widzenia sumaryzacji. Nie normalizujemy wyników długością zdania, ponieważ można przypuszczać, że dłuższe zdania będą lepszym wyborem.\n",
    "\n",
    "**Zadanie1 (2.5 pkt)** Uzupełnij kod automatycznej sumaryzacji:\n",
    "<ol>\n",
    "    <li>Uzupełnij funkcję **compute_frequencies**, która dla stokenizowanych zdań (lista list) wygeneruje slownik, ktory zwróci mapowanie słowo -> liczność wystąpień tego słowa w zbiorze dokumentów. Tokeny, które są stopwordsami lub znakami interpunkcyjnymi nie powinny być dodawane do słownika.</li>\n",
    "    <li>Zamień tekst na tekst pisany małymi literami, podziel go na zdania, a każde z tych zdań na słowa (pierwsze 3 linijki funkcji summarize)</li>\n",
    "    <li>Uzupełnij funkcję **create_sentence_ranking**, która na wejściu otrzymuje listę stokenizowanych zdań i słownik wygenerowany przez **create_frequencies**, a na wyjściu wygeneruje słownik mapujący numer porządkowy zdania na wartość istotności tego zdania (suma częstości tokenów tego zdania pobrana z freq)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* the international campaign to abolish nuclear weapons also offered to pay for kim's lodging with the cash received as part of its nobel peace prize ($1.1 million) it won last year \"in order to support peace in the korean peninsula and a nuclear-weapon-free world.\"\n",
      "* determining who will pay kim's hotel bill is one of many logistical issues still being hammered out ahead of the summit, including the aircraft kim will use to fly to singapore and the venue where trump and kim will meet, the post reported.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "stopwords = set(stopwords.words('english') + list(punctuation)) # stwórz listę tokenów, które powinny być ignorowane\n",
    "\n",
    "def compute_frequencies(word_sent):\n",
    "    d = defaultdict(int)\n",
    "    for sent in word_sent:\n",
    "        for word in sent:\n",
    "            if word not in stopwords:\n",
    "                d[word] += 1\n",
    "    return d\n",
    "\n",
    "def create_sentence_ranking(tokenized_sentences, freq):\n",
    "    d = defaultdict(int)\n",
    "    for index,sent in enumerate(tokenized_sentences):\n",
    "        sent_significant = 0\n",
    "        for word in sent:\n",
    "            sent_significant+=freq[word]\n",
    "        d[index] = sent_significant\n",
    "    return d\n",
    "            \n",
    "\n",
    "def summarize(text, in_how_many_sentences):\n",
    "    text_lowercased = text.lower()# TODO: zamień mnie na małe litery\n",
    "    sents = sent_tokenize(text_lowercased) # TODO: podziel mnie na zdania (np. z NLTK)\n",
    "    sentences_with_words_tokenized = [word_tokenize(sent) for sent in sents] # TODO: podziel zdania na słowa tworząc listę list (lista zdań, z których każdy element to lista tokenów w zdaniu)\n",
    "\n",
    "    freq = compute_frequencies(sentences_with_words_tokenized) # tutaj otrzymamy słownik, jeśli chcesz - wyświetl go - czy rzeczywiście częste słowa są tymi istotnymi?\n",
    "    ranking = create_sentence_ranking(sentences_with_words_tokenized, freq) # stwórz ranking zdań\n",
    "    sents_idx = get_top_n(ranking, in_how_many_sentences) # wybierz pewną ilość najistotniejszych zdań [ich indeksy]\n",
    "    return [sents[i] for i in sents_idx] # zamień indeksy na tekst\n",
    "\n",
    "def get_top_n(ranking, n):\n",
    "    return sorted(range(len(ranking)), key=lambda i: ranking[i])[-n:]\n",
    "    \n",
    "text = '''\n",
    "Washington (CNN) As preparations are underway for a US-North Korea summit, US officials are trying to solve the logistical issue of who will pay for North Korean leader Kim Jong Un's housing, according to a new report.\n",
    "\n",
    "A week after abruptly scrapping the summit with Kim, President Donald Trump announced Friday that the historic talks were back on for June 12 in Singapore.\n",
    "With its economy weakened from tough sanctions, Pyongyang is requiring that another country pay for Kim and his delegation's hotel bill, The Washington Post reported Friday.\n",
    "According to the Post, Kim is demanding to stay at the luxury, five-star Fullerton hotel, where a presidential suite costs more than $6,000 a night.\n",
    "America should be more at ease than this\n",
    "America should be more at ease than this\n",
    "White House and State Department officials declined to comment to the Post on the advance team planning details.\n",
    "Citing two people familiar with the talks, the Post reported that the US is open to covering the costs, but is considering asking the host country, Singapore, to foot the bill.\n",
    "The International Campaign to Abolish Nuclear Weapons also offered to pay for Kim's lodging with the cash received as part of its Nobel Peace Prize ($1.1 million) it won last year \"in order to support peace in the Korean Peninsula and a nuclear-weapon-free world.\"\n",
    "\"Our movement is committed to the abolition of nuclear weapons and we recognize that this historic summit is a once in a generation opportunity to work for peace and nuclear disarmament,\" ICAN International Steering Group member Akira Kawasaki said in a statement.\n",
    "The Post is also reporting that the US is expected to request a waiver of sanctions from the United Nations and US Treasury Department for expenses associated with North Korea's travel.\n",
    "Trump is expected to stay at another five-star hotel, the Shangri-La, which has hosted high security events before, according to the Post.\n",
    "Determining who will pay Kim's hotel bill is one of many logistical issues still being hammered out ahead of the summit, including the aircraft Kim will use to fly to Singapore and the venue where Trump and Kim will meet, the Post reported.\n",
    "The relatively secluded Capella hotel on the island of Sentosa is being considered for the site of the summit, people familiar with the talks told the Post.\n",
    "'''\n",
    "\n",
    "for s in summarize(text, 2): # wybierz 2 najlepsze zdania\n",
    "    print('*', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja relacji\n",
    "\n",
    "Zrozumienie tekstu wymaga zarówno rozumienia poszczególnych słów jak i relacji pomiędzy tymi słowami. O ile o znaczeniu pojedynczych słów mówiliśmy już trochę (embeddingi i ocena podobieństwa z ich zastosowaniem, a także problem POS-taggingu, który odkrywa jaką część mowy reprezentuje dane słowo), o tyle o relacjach między słowami  mówiliśmy niewiele. \n",
    "\n",
    "Relacjami między wyrazami w zdaniu rządzi gramatyka, dzięki której możemy zrozumieć jak wymienione w zdaniach idee łączą się ze sobą. Dotychczasowe badania w dziedzinie przetwarzania języka naturalnego zaproponowały tzw. drzewa zależnościowe (dependency tree lub dependency parse tree), jako wizualizację zależności gramatycznych między wyrazami w postaci drzewa. Korzeniem tego drzewa jest najważniejszy w zdaniu czasownik. Połączenia między węzłami w drzewie zależnościowym są etykietowane nazwami relacji między słowami.\n",
    "\n",
    "Wizualizacje generowanych drzew zależnościowych dla zadanaych zdań wygenerować można pod adresem: https://explosion.ai/demos/displacy\n",
    "\n",
    "Etykiety znajdujące się na krawędziach drzewa opisane są pod adresem: https://nlp.stanford.edu/software/dependencies_manual.pdf w rozdziale 2.\n",
    "\n",
    "Wizualizcję drzewa zależnościowego (bez etykiet na połączeniach węzłów) możemy uzyskać użyciem SpaCy i NLTK. Uruchom poniższy kod, aby zaobserwować rezultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "-----------------------------------\n",
      "        jumps                    \n",
      "  ________|______________         \n",
      " |        |             over     \n",
      " |        |              |        \n",
      " |       fox            dog      \n",
      " |    ____|_____      ___|____    \n",
      " .  The quick brown the      lazy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mary met Mike.\n",
      "-----------------------------------\n",
      "     met     \n",
      "  ____|____   \n",
      "Mary Mike  . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree # przydatne do wyświetlania drzewa\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # przykładowe zdania do przetworzenia\n",
    "\n",
    "def to_nltk_tree(node): # tworzymy drzewo\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.text, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.text\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    print(\"-----------------------------------\")\n",
    "    to_nltk_tree(sent.root).pretty_print() # stwórz drzewo i pięknie je przedstaw\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do czego może się przydać drzewo zależnościowe? \n",
    "Możemy wykorzystać takie drzewo np. do upraszczania zdań, odkrywania relacji między elementami zdania, czy np. do odkrywania do jakiego fragmentu tekstu odnosi się fraza nacechowana emocjonalnie (\"Bardzo lubię te babcine, wiejskie pierogi, ale dobrym kebabem w sumie też nie pogardzę\" => lubię--pierogi, nie-pogardzę--kebabem)\n",
    "\n",
    "Wykorzystajmy drzewo zależnościowe, aby stworzyć uproszczoną reprezentację zdania, zawierającą relację (czasownik) i argumenty tej relacji w formie relacja(argument1, argument2,...)\n",
    "\n",
    "**Zadanie 2: Prosta ekstrakcja relacji z wykorzystaniem drzewa zależnościowego**\n",
    "\n",
    "**Zadanie 2a (0.5 pkt)**: Wykorzystując atrybuty stworzonych przez spacy tokenów po uruchomieniu funkcji nlp() (https://spacy.io/api/token#attributes) - stwórz reprezentację CONLL, w której znajdą się następujące atrybuty (kolumny):\n",
    "<ol>\n",
    "<li>identyfikator słowa w dokumencie</li>\n",
    "<li>tekst słowa</li>\n",
    "<li>etykieta z drzewa zależnościowego na połączeniu z \"rodzicem\"</li>\n",
    "<li>tekst rodzica z drzewa zależnościowego</li>\n",
    "<li>listę dzieci z drzewa zależnościowego</li>\n",
    "</ol>\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "\n",
    "<pre>\n",
    "0 The det fox []\n",
    "1 quick amod fox []\n",
    "2 brown amod fox []\n",
    "3 fox nsubj jumps [The, quick, brown]\n",
    "4 jumps ROOT jumps [fox, over, .]\n",
    "5 over prep jumps [dog]\n",
    "6 the det dog []\n",
    "7 lazy amod dog []\n",
    "8 dog pobj over [the, lazy]\n",
    "9 . punct jumps []\n",
    "\n",
    "\n",
    "10 Mary nsubj met []\n",
    "11 met ROOT met [Mary, Mike, .]\n",
    "12 Mike dobj met []\n",
    "13 . punct met []\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The det fox []\n",
      "1 quick amod fox []\n",
      "2 brown amod fox []\n",
      "3 fox nsubj jumps ['The', 'quick', 'brown']\n",
      "4 jumps ROOT jumps ['fox', 'over', '.']\n",
      "5 over prep jumps ['dog']\n",
      "6 the det dog []\n",
      "7 lazy amod dog []\n",
      "8 dog pobj over ['the', 'lazy']\n",
      "9 . punct jumps []\n",
      "\n",
      "10 Mary nsubj met []\n",
      "11 met ROOT met ['Mary', 'Mike', '.']\n",
      "12 Mike dobj met []\n",
      "13 . punct met []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree # przydatne do wyświetlania drzewa\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # przykładowe zdania do przetworzenia\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        index = word.i\n",
    "        text = word.text\n",
    "        et = word.dep_\n",
    "        ancestors = [t.text for t in word.ancestors]\n",
    "        anc = ancestors[0] if len(ancestors)>0 else word.text\n",
    "        children = [t.text for t in word.children]\n",
    "        print(index, text,et, anc, children)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że najistotniejszym czasownikiem jest słowo \"jumps\" (korzeń drzewa zależnościowego (ROOT))\n",
    "Widzimy też, że słowa grupują się odpowiednio. Dzieci słowa 'fox' to ['The', 'quick', 'brown'] - a więc określenia definiujące jaki ten lis jest! (Podobnie dla słowa dog)\n",
    "\n",
    "\n",
    "**Zadanie 2b (2 pkt)** Ekstrakcja relacji.\n",
    "\n",
    "Wiedząc jak należy pobierać informacje o drzewie zależnościowym z obiektów typu Token w SpaCy, napisz funkcję parsującą, która dla każdego zdania (zdania przetworzonego przez SpaCy) wyekstrahuje najważniejszą relację (czasownik będący ROOTem), a także argumenty tej relacji (podmiot i dopełnienie) bazując na wygenerowanym drzewie zależnościowym.\n",
    "\n",
    "<ol>\n",
    "<li>Relacja powinna zostać zapisana w zmiennej predicate</li>\n",
    "<li>Podmiot, zdefiniujmy jako token ze zdania, który połączony jest z ROOTem relacją 'nsubj', zapisany powinien być w zmiennej subj.</li>\n",
    "<li>orzeczenie zaś określone może być np. jako:element połączony z ROOTem relacją 'dobj', bądź, jeśli ROOT nie ma połączenia 'dobj', a połączony jest z elementem relacją 'prep' (przyimek w relacji do czasownika), to orzeczeniem jest token, który połączony jest z tym przyimkiem relacją 'pobj'. Jeśli występuje sytuacja druga, tzn. przyimek jest połączony bezpośrednio z ROOTem - a dopiero ten przyimek z określeniem, przyimek powinien zostać doklejony do napisu zapisanego w zmiennej predicate (Dla uproszczenia załóżmy, że przyimek występuje zawsze po czasowniku). Dopełnienie zapisz w zmiennej 'obj'.</li>\n",
    "</ol>\n",
    "Aby zrozumieć działanie dopełnienia - spójrz na oczekiwany rezultat tego zadania i na drzewo zależnościowe wygenerowane w pierwszym fragmencie kodu tej sekcji.\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "\n",
    "<pre>\n",
    "jumps over(fox, dog)\n",
    "met(Mary, Mike)\n",
    "</pre>\n",
    "\n",
    "O ile drugi przykład met(Mary, Mike) jest oczywisty, to pierwszy powinien zidentyfikować słowo 'jumps' jako relację, zauważyć, że nie istnieje bezpośrednie dopełnienie (brak 'dobj' dla roota), za to mamy przyimek over, który to z kolei jest połączony z oczekiwanym dopełnieniem ('dog'). Zatem przyimek doklejamy do nazwy relacji, zamieniając dotychczasowe jumps na jumps over, a dopełnieniem staje się element połączony z przyimkiem relacją 'pobj': dog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps over(fox, dog)\n",
      "met(Mary, Mike)\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree # przydatne do wyświetlania drzewa\n",
    "                                    # ładujemy modele SpaCy\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # przykładowe zdania do przetworzenia\n",
    "\n",
    "def parse(sent):\n",
    "    predicate = None\n",
    "    predicate2 = None\n",
    "    subj = None\n",
    "    obj = None\n",
    "    for word in sent:\n",
    "        if word.dep_==\"ROOT\":\n",
    "            predicate = word\n",
    "            for child in word.children:\n",
    "                if child.dep_==\"nsubj\":\n",
    "                    subj = child.text\n",
    "                    break\n",
    "            for child in word.children:\n",
    "                if child.dep_==\"dobj\":\n",
    "                    obj = child\n",
    "            if(obj==None):\n",
    "                for child in word.children:\n",
    "                    if child.dep_==\"prep\":\n",
    "                        for child_ in child.children:\n",
    "                            if child_.dep_==\"pobj\":\n",
    "                                obj=child_.text\n",
    "                                predicate2=child.text\n",
    "                                break\n",
    "                                \n",
    "                            \n",
    "            break\n",
    "    # TODO: wykrywanie podmiotu, dopełnienia i głównego czasownika z drzewa zależnościowego\n",
    "    \n",
    "    if(predicate2 != None):\n",
    "        print(\"{pred} {pred2}({subj}, {obj})\".format(pred=predicate,pred2=predicate2, subj=subj, obj=obj))\n",
    "    else:\n",
    "        print(\"{pred}({subj}, {obj})\".format(pred=predicate, subj=subj, obj=obj))\n",
    "    \n",
    "for sent in doc.sents:\n",
    "    parse(sent)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "W ramach laboratoriów dotknęliśmy wielu różnych problemów NLP:\n",
    "<ol>\n",
    "    <li>Lab1: Szybkie wyszukiwanie/operowanie na tekście -- wyrażenia regularne</li>\n",
    "    <li>Lab2: Klasyfikacja (Tworzenie reprezentacji BagOfWords, Tokenizacja, Lematyzacja, Stemming, normalizacja TF-IDF, SVM, Naiwny Bayes)</li>\n",
    "    <li>Lab3: NGramy (Reprezentacja NGram vs BagOfWords, detekcja języka, generowanie tekstu)</li>\n",
    "    <li>Lab4: Embeddingi jako niskowymiarowa alternatywa dla BagOfWords/NGram (podobieństwo w przestrzeni embeddingów, embeddingi do klasyfikacji) + Poprawianie literówek (odległość Levensteina)</li>\n",
    "    <li>Lab5: Sieci neuronowe (Przypomnienie z SI, sieci jako sekwencja operacji na macierzach, sieć implementowana bez użycia frameworków)</li>\n",
    "    <li>Lab6: Tworzenie zasobów (Crawling/Scraping danych z sieci, tworzenie zasobów z użyciem reprezentacji CONLL)</li>\n",
    "    <li>Lab7: Sieci rekurencyjne w przetwarzaniu tekstu (RNN od podstaw bez użycia frameworków, idea historii w RNN)</li>\n",
    "    <li>Lab8: Detekcja sentymentu z użyciem zaawansowanych architektur sieci (GRU/LSTM/CNN)</li>\n",
    "    <li>Lab9: Wykrywanie encji nazwanych i fraz rzeczownikowych</li>\n",
    "    <li>Lab10: Sumaryzacja poprzez wyszukiwanie zdań kluczowych (Key-sentence extraction), modelowanie tematów (topic modelling - LDA), Ekstrakcja informacji (Drzewo zależnościowe, rozbiór gramatyczny zdań). </li>\n",
    "</ol>\n",
    "\n",
    "Użyliśmy także szeroko używanych narzędzi takich jak:\n",
    "<ol>\n",
    "    <li>NLTK - natural language toolkit - narzędzia do popularnych zadań NLP</li>\n",
    "    <li>SpaCy - narzędzia do popularnych zadań NLP, młodsza alternatywa NLTK</li>\n",
    "    <li>sklearn - narzędzie implementujące algorytmy uczenia maszynowego</li>\n",
    "    <li>Keras - wysokopoziomowy framework do sztucznych sieci neuronowych</li>\n",
    "    <li>gensim - narzędzie do modelowania topików</li>\n",
    "    <li>Pandas - narzędzie do łatwego operowania na danych</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "**Ponieważ chcielibyśmy, aby ten przedmiot stawał się coraz lepszy przygotwałem krótką ankietę dotyczącą przedmiotu, będę wdzięczny za udzielenie w niej odpowiedzi! https://goo.gl/forms/jWnSAwHkzKKs86QV2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
